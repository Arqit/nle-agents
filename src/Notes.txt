To Do

## eventually, double check with our original DQN implementation to ensure that nothing has bein left out? --> Do below + check how would new configuration perform
# If I have time, try to augment the prioritized experience replay with Pong to see if it is working properly (do tomorrow!) PER and Huber Loss (Smooth L1 loss)
# Look into proper DQN implementation to get an effective architecture (Others dont seem to be as fancy with the kernel_size and stride... In our case, I dont think it is necessary)
# 0 is giving the "Unknown ^M command" error
# We'll probably take some inspiration from their train function ( in terms of the loss function used, the learning rates + other hyperparameters) + Check DQN_Zoo for some inspiration
# Try to get on Colab! (For sake of train and sanity!)
# PER seems to be working, now just need to find some form of evaluation method (which will help to tune the hyperparameters for PER)  --> Focus on factors such as lr, replay size,etc
# Check if DQN's need BatchNormalization
# Properly configure the kernel size and stride!
# We cant just be decaying the exploration since we always need to be learning (it should be high and ever so slightly decay) but should always be non-zero (Check how will Noisy Layer handle this)


Notes
# I cleaned out some of the Cropping stuff since it was coming in the way and I highlly doubt we will be using it?

# Will definitely have to mention the use of the experience replay buffer: It decorrelates samples which stabilizies the training
# https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf + the other 2 provided papers
# The PER Paper: https://arxiv.org/pdf/1511.05952.pdf  --> Try to do the results like this paper
# We want to replay the transitions that we expect to learn the most from
# Double DQN stabilizes training
# Find out from the pros if just the world as is is good enough (many implementations seem to have an associated temporal aspect)
# Smooth L1 loss behaves like MSE for small error and like mean absolute error when the error is large


PER Notes
# My Prioritized Replay Buffer Sampling # Read the other PER notes in MyAgent.py
# I increased the replay buffer size from 10000 to 1000000... That is what the article states!
# In particular, we propose to more frequently replay transitions with high expected learning progress, as measured by the magnitude of their temporal-difference (TD) error.
# The magnitude of the TD error (squared) is what we want to minimize in the Bellman equation. Hence, pick the samples with the largest error so that our neural network can minimize it!
# The idea is to improve on the samples that we performed badly on (as measured by the TD_Loss)
# The goalof PER is to target the weaknesses to improve on them in the long run... This is conflicting! Investigate!!

World notes
# The world is zeros everywhere and is only populated where our world actually is
# From empirical evidence, the reward function varies dramatically, its typically between 0 and -0.01( which is used as a penalty)
# For positive rewards, it ranges from 2 to 20, so its not a good idea to clip the rewards(I think it should be sensible to maintain that variability)
# At the moment, specials is just a zeroes array???
# Theres definitely a difference between glyphs and chars - glyphs is zero when undefined and contains large numbers. char is 32 where undefined and has small numbers
# Remember that the size of the world is 21 x79
